0:00
Host
Hi, everyone. Welcome to our event. This event is brought to you by the rock club, which is a community of people who love data. We have weekly events and today is one of such events. If you want to find out more about the events we have, there is link in the description, click on that link and you'll see all the events we have in our pipeline. Then do not forget to subscribe to our youtube channel if you don't want to miss out on any future themes because they will be as awesome as the one we have today and we have a great Slack community where you can hang out with other dating into that. So don't forget to check it out too during today's interview, you can ask any question. There is a pinned link in the live chat. So click on the link, ask your questions and we will be covering these questions during the interview. And typically this is the time when I stop sharing my screen, but I want to add a bit more. So this podcast three is actually sponsored. We don't often have sponsored interviews. So today is a special interview especially for me too and for the community and the interview is sponsored by Vector Hub. So I included the link in the description. You can check it out. And basically here you can find everything about vectors and databases. And this is also the topic today. So you will learn more about that but check the link too. And thanks Vector Hub for supporting this. Now, I will stop sharing my screen and OK, I now have the questions and Daniel, if you're ready to start, let's start. 

1:45
Guest
Yeah. 

1:47
Host
OK. So this week we'll talk about building production search systems and we have a special guest today, Daniel, Daniel is an entrepreneurial technologist with a 20 years career. He is a co-founder of Super linked.com. And what we saw Vector Hub is one of the projects by Super Link. So it's a machine learning infrastructure start up that helps build information critical systems from recommended engines to enterprise focused LM apps. And before starting his own company, he was a tech lead for machine learning infrared youtube ads. So welcome Daniel. 

2:25
Guest
Hey, hey, happy to be here. Thanks for having me. 

2:29
Host
So before we go into our main topic of building search systems, let's start with your background. Can you tell us what your career journey is for? 

2:38
Guest
Yes. Um Very happy to, yes. This kind of uh 20 year time horizon is uh maybe a little bit of an exaggeration. But uh uh basically I'm, you know, originally from Slovakia, this kind of uh typical Eastern European uh technical background of sort of coding competitions in high school. Um I've been through a couple of internships during university with uh Google and IBM research, um eventually decided to start my own company, the first one out of university. Uh because both of those companies were very big and it took months to launch new things. Um So I started my first start up already uh after I, I moved to Switzerland, um and then eventually ended up big at Google, uh as you said in uh youtube ads, uh building systems that predict uh ad campaign performance. So when people buy these ad campaigns, uh they like to get feedback of, OK, if you run this campaign, you'll get 1000 clicks or something like that and then they see the forecast, they, they tweak it. Um And then once they are happy with the forecast, they buy it. Uh and eventually, uh yeah, uh the, the systems I was working on uh powered or power. Um All of youtube ad buying which there is uh reservation ads and auction ads. And, you know, it's uh the ads world, it's uh not so transparent for many people, but uh it's, it's, there is a lot of uh rabbit holes you can fall into. Um and then towards the end of 2018, I left and, and started, you know, working on my second start up and we went through many different ideas and, uh, a couple of years, uh, into that process, we landed on, uh, Super Linked and, you know, that's the, um, project or company. Uh, I'm, I'm working on now 

4:43
Host
and I loved how casually you dropped this. That, ah, yeah, I just have this typical eastern European background where people do competitive programming at school. It's like, 

4:56
Guest
seriously. It's cool. Yes. So, in, I think about second year of uh high school, um I had this kind of uh realization that if I don't do something uh like competitive programming, it will be very difficult to kind of leave the country and uh work on some interesting problems. Uh So I always did some math uh kind of competition but competitions, but uh basically programming seemed like the thing, you can actually get a job, right? Um And so, and so I decided to take it more seriously. And um yeah, eventually it did lend me those internships and uh but it was a few years kind of uh journey of getting up at 3 a.m. and doing topcoder competitions that are, that were kind of uh us time friendly and also meeting lots of people um who eventually ended up all in California, optimizing the ads kind of with, with me in one way or another, right? So that cohort of the competitive programming folks uh is a small one and I kind of keep running into uh those people still it is fun. 

6:10
Host
Mhm. And uh it's probably still easy for you to pass all these programming interviews at Google Meta and the like. Right. 

6:19
Guest
Um Well, luckily I didn't have to do it for quite a while um in, in that sense, but you would be maybe surprised how many of such uh problems I encounter quite often now that we work at the infrastructure focused uh product. Right? So all of those tasks that maybe for many people are hard to connect to real work, uh you know, doing some rotations of uh uh three data structures and things like that. Um Actually, if you work on infrastructure and these heart problems, um you find many applications uh for, for those types of uh you know, like dynamic programming, for example, has been uh always a little bit uh out there, right, in terms of applications. Um But actually, uh now we have this uh chunking problem uh as a part of the surge uh problem overall. And uh I find like certain types of tracking can be solved by dynamic programming. So it is all kind of uh comes together in the end, which is fine. 

7:35
Host
I added a note because this is something I want to ask you. But probably towards then because we have like quite a few questions that I cover first. And since we talk about production search systems today, so what is actually search 

7:52
Guest
uh depending on how philosophical you want to get uh here. Not too much, not so one way you can frame search is uh actually a decision problem. Uh So you have a lot of information somewhere and you want to decide which pieces of that information matter in a given situation, right? Which, which are the most relevant pieces of that information uh to, to, so that's kind of the first decision and then you are typically kind of uh navigating some broader problem, right? Uh So somebody uh wants to buy something on the internet um or discover their next article to read. Um Or you are looking for a machine part that is required to fix the machine, right? Like all of those situations probably contain information retrieval uh sub problem in there somewhere. Um So yeah, isolating relevant data within a bigger pile of data and then we can talk about what is relevant and what is a pile, right? 

9:03
Host
Yeah. So I ask you about search and in your answer, you mentioned the information retrieval system, which is like the same thing, right? 

9:11
Guest
Basically, basically, uh by the way, these boundaries are often quite arbitrary um of like where the recommender systems start and and personalized search uh or, or ends, right? For example, or now we have the retrieval augmented generation like how is that any different at all? Um Yeah, I think we try to put some buckets of functionality just to make it easier to talk about it. But at the end of the day. Yeah, like information retrieval is the common name for the field. Um And maybe one other kind of keyword when people are searching for learning materials, I would say it's a representation learning, right? Uh So that's a domain of machine learning where we are trying to uh build models that uh help us encode this information we are uh trying to find or search through, right in a way that makes it more efficient uh to, to search it. Right. Basically, 

10:13
Host
then I, I think in Berlin, there is a conference called Haystack which is about search and I think that the the the reason they use it because like you have a pile of grass. Hey, right. And then there is a needle there and you want to find it, right? So this is the search problem. Uh So this pile of information, this is what you have like the internet and then the needle is what you want to find, right? Because you can see through the entire pile, you need smart method of finding the needle like a magnet or whatever. 

10:45
Guest
Yes, because usually there is a constraint of like finite amount of time, right? So if we had the forever, then we could always go through the whole pile and uh very carefully look at each blade of grass. But typically there is a constraint of uh you know, they say that every 10 milliseconds uh added to um shopping interface uh significantly impacts the revenue, right. So uh and, and that these kind of situations repeat across many domains. So latency is kind of a big factor and you know, search existed like the problem existed since the beginning of kind of computer science, right. So uh these concepts are not new at all. 

11:29
Host
Yeah, I think there is a paper about vector spaces which is like from seventies where they explain this bag of words. And yeah, I wanted to talk about uh um so there is a very good book which is called Informa Introduction To Information Retrieval. I think one of the authors is is Christopher Manning or Christopher. Like this is a very good book and this is one of the books I started my journey into an LP with and this is super well written and there the idea is that you have a document, document contains words and then you have a phrase, let's say the document is about, I don't know uh search, right? And then we look for a word search, right? And then it's OK, like this document contains a word search 1000 times must be relevant. Let's show it right. So this, so the book is about mostly is about this um text search, right? Yet today we talk so much about vector search vector and beatings and all of that like what are these things and how they are related and how this vector search is relevant to what I mentioned to the tech search. 

12:45
Guest
So if we kind of uh talk about search world before uh vectors became uh very popular. Um And they have always been there, right? I I think the difference is have they been used in production systems? Right? Or did people keep the production systems a bit simpler? Um until recently, uh which I think is the case. So for previous generation of search systems, right, you try to build data structures that helped you find uh you know, there is always a concept of query and then you want to do things to this query to prepare it for uh the evaluation, right? So you might uh expand the synonyms, you might rewrite the query to make it efficient, more efficient to evaluate. Um And you will end with some object that describes your objective, like what do you want, right? Um And then on the search index side, right, there is this whole field of kind of infrastructure work uh in which you have your haystack, right? Do you want to um process it? You want to ingest it and build some kind of data structure on top of it, we call index which um makes it very efficient to take that query object and match it against uh the, the whole haystack but like as fast as possible, right? And so three vectors uh the the core idea of that uh index structure was this uh reverse list right, where basically you have a list of keywords with pointers to where those keywords uh appear in the orange cubit. Um And you know, those keywords can be kind of the types of the words, they can be all keywords, they can be normalized in different ways. But that's kind of the basic idea, right? You make kind of like a dictionary. Um and then you use that to match the queries to the, to the text. And the, you know, obvious problems are uh this kind of uh brittleness of the setup, right? Because you rely on very specific handcrafted uh heuristic of how to create the dictionary and what to do to those queries to kind of match the two sides together. Um And by the way, this is still in the realm of the underlying kind of retrieval step, right? So we should probably also mention that there are kind of usually two distinct steps in each of these uh search systems. So you have this uh initial retrieval step, uh you can call it can generation where you want to like quickly figure out. OK. Here are the uh potential good results. Um So you are narrowing the whole haystack to some uh tiny fraction of it. Um And then you have the second step of ranking, right? People consider these problems very differently. Uh And the ranking step um in its final most sophisticated form, you can think about it as a machine learning problem that's usually framed as OK for this query. Uh And this potential candidate result, um what is the probability that they actually uh match? Right? Uh So it could be, what is the probability that the user that's doing the search actually clicks on, on this particular result, right? There are different ways to frame the problem. But this, this part sounds more like machine learning and the first part sounds more like engineering, right? So we had kind of in a way stupid uh retrieval and smart uh ranking. Um And then in the drinking step is usually where those models get complicated. You have all the envelopes issues. Um That's where you bring all that context into the picture, right? And you try to run that model and all the candidates and reorder them and serve that to the user or to the system that's doing the searching. So, so that's the anatomy. You have those two parts usually. Um So I 

16:45
Host
have this book, those who watch this in the record, listen to this in the recording. Don't see this is a Ger German Grammatic grammar book. And yeah, so like there is a lot of information and let's say I want to find something in it, right? So how would we build a search system for that? So we need two steps, right? You mentioned like this generation and binary. But also we even before that, we need to index the entire book, right? Built the index. So how would we go about that step by step? 

17:20
Guest
Yeah, I mean, uh you're basically actually at the end of that book that is most likely uh such uh look up table, right? Uh Maybe called the reference or maybe this one doesn't have it. But uh it does, 

17:34
Host
it only has a table of contents but some peop some books do. Yeah. 

17:39
Guest
Yeah, some, some books do. So, I mean, the, the practical answer is uh use uh Lucy, right? Or one you know, there are uh big open source projects out there that um help you solve this problem of interesting whole bunch of documents uh cutting them up and building that uh uh that index structure. Um And these things are built into databases now they are also exist as stand alone uh solutions. So yeah, you should really, really not build uh reverse kind of keyword. Look at 

18:20
Host
this in term it called register which is and then like it's uh like or a which is but like go to page 108 for word gi and go to page 84 for noni to go to page 126 

18:39
Guest
right? Yeah. And you will notice that that an interesting aspect of this page is that these words are usually ordered alphabetically, 

18:47
Host
right? They are. 

18:48
Guest
Yeah. Yeah. And so that's like a very basic way to make it easy to find the word in the list is you can basically binary search for an index in that list, right? Uh If you know that's the data structure that works for a person looking at the book. Uh Normally you would have some kind of uh three structure where you go down a tree following the letters, right? So uh the the uh you would have the first letter and then the second letter and as you go through that word, you follow down the tree and eventually the leaf of the tree uh or the node where you end would have a list of places where uh the the kind of keyword matching the graphics that took you to that node um exists in the original data, right? Uh And then the whole game is like, how do you keep this thing updated when you in just new documents? Um But again, uh Lucine is your friend for this. Um And so OK, so we, we talked a little bit about the first part of your question, right? Like how uh you know, if you parachute into uh uh the two thousands uh kind of this is the stage you've seen, right? Yes. Uh And now the question is OK, why do we need something new? Right? The the deficiency of the system is um I would focus on maybe two elements. One is that it's brittle. So it relies on very specific forms of these keywords appearing both in the query and in the original document even though you may expand the synonyms. Uh the practical reality of managing a search system in production is that you have very many special cases and very long configuration files that help you. I want to 

20:37
Host
use English for searching for something in the German book, right? 

20:42
Guest
I mean, that's, that's, yeah, that's all another type of problem. Uh You could consider the synonym expansion to maybe go cross language, right? Um But there is always a set of queries that you will find in your logs where you didn't return any results, for example, or return the wrong results. Uh So the day to day life of a search relevance engineer um is to kind of look at that log uh somehow figure out which type of queries uh make the most impact and are still handled poorly. And then you go and you try to create rules and um you know, you try to address that uh problem uh and this increases the complexity of your system and then this kind of goes forward and then this person quits and the new one joins and sees all the rules and, you know, it's, it's kind of layers of complexity. Um So, so that's kind of the first problem, right? It's very brittle and very kind of uh heuristic uh based. Um And then, you know, there is the other side of the problem which is OK. But in reality, we rarely just have text, right? Um We don't we rarely have just documents. Uh There are, there are pictures there. Um uh if you imagine uh database of the enterprise company uh with, you know, hundreds of columns somewhere sitting in bicycle or post that this company literally runs on and it's a critical table, right? Some of these columns will be strings uh probably, but there will be all kinds of other things in there, right? Uh And then you have your data warehouse with all the locks, right uh generated by your infrastructure by our users. Um So that's kind of the data reality, right? And then you somehow want to do retrieval that uses all the data. So that's kind of the second part of the problem. How do you kind of go beyond just matching some strings to strings? First specific way you encounter this often is personalization, right? OK. So we have some users, we have some data about these users and maybe they send us a search query or maybe they just show up on the website or in the app, how do we show them the product they want to buy or the document they want to read or you know all of this stuff, how do we combine the behavioral data with the content? Um And this typically happens in the ranking step. So that's why it's so complicated. Um And, and you have all the kind of problems, machine learning type problems there. Um So I would say this is the state of the world and now we can kind of switch to vectors and, and talk about the difference, right? Like what, what's, what's what's going on? Right? Um I would say the the first problem that gets addressed is this uh brittleness, right? Like how do we make this problem of matching the query to the indexed object more robust? Right? So that when we say a manager uh and the document says uh whatever uh leader there will be a match, right? Um Yes, you can handle this specific case with the synonym expansion. Uh But there is basically infinite such cases. Um And so how do we make it more robust? And the idea is that OK. So instead of trying to figure out all the possible rules in which we can match the words, what about instead we come up with some representation that will exist in the middle of those two kind of documents and queries um where we will kind of project both sides into this shared the presentation and this projection will be more robust, right. Um And that's what basically embedding models help us do is do this kind of projection such that when things on the input of the projection are similar for some definition of similar uh they will end in a similar place that the representation. And that's this representation is vectors, right. Um So in a in a way vectors make that initial matching candidate retrieval problem more robust, right. And that then scales across modalities because it turns out that we can uh index on one side uh images into this representation. And then from the other side, we still uh uh embed queries, text queries, right. So now you are matching text to image somehow through this uh common uh place in the middle. Um And in principle, you can kind of make do this for anything, right. In principle, you can kind of take all kinds of data on the left side, uh all kinds of context on the right side, not just the text query, but also the history of the user, whatever it might be. Uh And you can kind of have a model uh that encodes or you know one model for the left side, one model for the right side and then uh they encode those two pieces in the middle and then you do the matching and you know the whole kind of hype around vector databases uh comes from this that like that matching and doing it very efficiently uh seems pretty important, right? Um So, so that also kind of helps you understand that OK, like in this new world of vector based search or dense vector based search, there will probably be two main problems, right? Uh One is how do you make vectors from data such that those vectors represent the different properties of your data that you care about and then how to index and match those vectors very quickly. So there will be some compute problem and there will be some kind of search slash database problem, right? Um And, and that bro broadly is how I think people should think about the space. Um And just kind of finish that thought. Uh you know, we managed, we mentioned uh kind of my newest uh start up that uh you know, I and a bunch of clever people have been working on for the last couple of years. We work on the compute problem, right? So we are not building a vector database, we actually work with vector databases because uh the idea is that together we can solve those two parts of the problem. Uh And then the end client gets the solution that both can do the compute and can do the search. 

27:21
Host
What do you actually mean by the computer? So maybe I'll take a bit of a step back because like there was quite a lot of information and I want to make sure I understood it. So if I go back to my German grammar book example, so previously we would index each page or maybe each part, each section uh of the book with an uh word index, right? So put this into Lucine and then Lucine would like, we would have a bunch of rules. But basically for each word that I have on the page here, I could have a link to that page. Or to that section, right? So if I'm interested in uh um I don't know the word but upper then I know that I need to go to section two unit two of the book, right? That works up to some point when there are so many rules, synonyms and all that. So for example, what if I want to use English to search for German or Russian or Slack Hand, right? Then like we cannot uh infinitely expand like our indexing include all these uh synonyms and other languages. 

28:34
Guest
For example, you would want to search for swear words. Like somebody comes and says like, OK, what are the swear words? And uh that concept doesn't necessarily exist in your index but it exists in how we understand languages, right? So you know, you need some strategy for handling that query and that's what yeah, I think it becomes quite obvious that you can't anticipate all these questions that people might be asking. 

29:00
Host
So then we can we come up with some sort of numerical representation for each word of document and basically each document becomes a large um array of numbers, right? Such that if two things are similar, then the numbers are similar. So for example, I have a query which is but in English and then I have a section unit in my book which talks about this prepositions, right? Then they would have similar representation, right? So then we have this a different way of looking for information than each word. Each document is a sequence of numbers using an array of numbers. And then we use vector data basis to store the documents, right? And then the document would say, OK, you need to go to page 110 and read about that. All right. So this is how we would do now with vector data devices. And then you mentioned compute, OK. I understand what a vector device is, right. So this is I think then that stores vectors, right? And then I have a vector and then I say OK, I need to find top 10 vectors that are similar to this vector. Give them to me, right? So this is what the vector database is doing. But what exactly is the computer that you mentioned? 

30:21
Guest
Right. So uh you have two places where you are uh running some models, right? Uh The the first place is the injection into the vector database, right? You have some documents somewhere. You need to compute all the vectors that correspond to these documents. And maybe when the documents change, you need to recompute these vectors. Um And maybe when you want to use the some meta data of these documents, like for example, which documents are people actually clicking on, right? Or when were these documents created? Um This creates some kind of data landscape on the input and then you have your vector database on the kind of destination and somehow we need to connect uh these two sites. So there will be some data engineering work happening, right? Some kind of pipeline work. Uh That's half of the problem. And the other half of the problem is the modeling work like OK, how, which kind of models are we running on this data? Um You know, how are we uh rolling out new model versions? And do we need to recompute the database when we do that or not or partially? Uh So we have this kind of ingest ingestion problem where there is kind of a big compute component, uh Basically running models on some data. And then there is the query handling uh path right where you have, let's say uh your user uh who uh put in a query and you need to also turn that into a vector so that the database can match those two vectors against each other, the kind of document part vector, let's say with the creative vector. Uh So you need to on in both of those pathways, basically construct vectors um from some inputs. Um And of course, these are kind of different requirements because for the injection, maybe you can batch these workloads for the query handling, maybe you want to be fast. Uh But somehow always you need to be consistent because you are landing into the same vector space uh because you want to be matching those two sides together. Uh So those are the two instances of the what we call the vector compute problem. 

32:43
Host
Yeah. So if I talk about a specific example, if I think about a specific example, so there is a model from open eye called clip and this model, what it can do, it can turn text into vectors and images into vectors inside. You have a way that you can use text to look for images. Like you can just write, I don't know, blue, wait black cat. And then you would get images of black cat, right? Black cats, right? That's right. And let's say we used clip or we use the usual, I don't know, sound way of embedding. Let's say we used uh bird for embedding our uh book, right? So for embedding all the words uh creating this vector and indexing, right? But then we heard about this clip and we thought OK, we also have images in the book. Now we want to switch our embedding strategy, we want to use a different model for embedding, right? And instead of rewriting the whole pipeline, if we use like a special framework for creating this pipeline for indexing, re indexing and all that, right? So for us placing one model with another and add images would be much easier if we use a frame, 

33:59
Guest
right. Yeah, that's exactly right. And this we are still in the kind of basic level of this uh problem because you, you kind of used an example where we just replace the old model with a new one, right? Um The thing is that in practice, it's not so easy. Um At some point, basically your data that you want to process in this way just starts to not just be one big string or one image, right? And you start to have this, uh you know, let's say your pro product manager comes in and says, hey, the search that we have built for our news website when I type car, I'm getting results which are kind of uh too old, right? Uh And we are news website, we need to have kind of fresh results, right? Uh And then, OK, then what do you do? Right. And um one of the concepts of how people deal with this kind of stuff, it's called hybrid search, right? You start to combine OK, I'll have my kind of uh vector uh similarity search and I layer on top of it uh some other constraints and I say, OK, pre filter all the news articles that are newer than uh one month and then within those uh match with the kind of vector proximity to the query, let's say, um now the problem with that is that what if there is a super relevant article that's 32 days old, right? You will miss it. Um So the, the and then of course, there are many such instances, right? The product managers are creative, they come up with all kinds of constraints. And if you layer all of these in this kind of classic kind of like uh waterfall of constraints type model, uh it will over constrain your results and it will ultimately not actually work for the end user. Uh Because what the end user is looking for is some combination, some uh compromise, right? I want kind of new stuff but also relevant and probably a good idea from the recommender engine uh side of things is like some of these results should be quite popular or maybe popular for people like me. Uh And then you get into this kind of complicated uh and also real popular, right? Yeah. Yeah. So, so this is the real world, right? Like that's kind of how yes, you, you start with this like OK, let's uh embed some text, let's embed the query, let's match the two maybe use another language model to reorder uh the results because that can be then refining the match between the query and the document. Uh But that whole uh system will still just take the text part of the data into account. And you know, as I said, in my examples that you very quickly run out of uh kind of uh the levers to uh move to actually get to a result that you can run in production and it can power a sign significant significant part of your um product at scale. And what people do then is they go custom, right? Like OK, custom embedding models, custom ranking models, uh you know, tight torch uh all the associated with kind of uh alos problems. Again, how do we have enough data? How do we train this thing? Uh Now should we train embeddings for each retrieval task separately or have some general embedding then maybe have ranking model separate for these cases. And that's where we are looking at kind of 6 to 9 months project with some ML data science folks. Um And that precisely is the point where you should come talk to us because we have a way to embed complicated data, but we pro productive that process, right? And so uh we, our goal is to make it much easier to deal with those more complicated uh situations um and make it not take nine months, right? 

38:10
Host
So you said that it's possible that you have multiple embeddings for a single document, right? So there could be embedding for, I don't know titles embedding for content embedding, for images embedding for some parameters. And then like you have maybe five embeddings then as a lot of extra meta information like popularity tags um like where the user clicked or clicked similar items, I don't know, like all this kind of stuff, right? And then it's all there in database or databases. I don't think you need to link it together somehow. 

38:50
Guest
Exactly. Exactly. And ideally at the end of the day, you have for your articles or pieces of the articles and your users one vector each and this vector encodes everything, you know, right. Uh All the information that you have about your articles, all of the stuff you named somehow needs to eventually make it into the article tank vector and everything you know about your user uh needs to make it into the user preference vector. Um And then the question is OK. How so you, and then you know, and then you have the etl problem of make it in there in terms of, you know, getting the data from where it is out and into your kind of processing pipeline, right? And then the modeling problem of OK. How do we deal with those clicks, those categories, those uh separate vectors. How can we bring it all together? 

39:49
Host
Mhm And um yeah, I I know that in Lucine. So we talked about this problem of frequency, right? So what if we are a news website? And it means that we want to show something that is recent? But what if there is a super relevant article related to my search that is more than a one month old, right? And I know in Lucine there are these type of queries that filters shoot and bust like uh you can say the article must be at least less than one month old, right? And then it would just filter completely altogether all the old articles, right? Or you can say it's true. And then if it's super relevant, then Lucine would still bring it up. But when it comes to vector databases, uh I don't know if they can have this sort of functionality, right? So does it mean we need to always have multiple databases when we want to, to do things like that? 

40:48
Guest
So that's an interesting question. And our view at this problem is that we, we believe that you can uh basically replicate a lot of that shoot type functionality in kind of pure vector form. Uh So you can basically say, hey, I want relevant result towards this query and bias it towards what the type of stuff the user clicked on before and also bias it towards like popular stuff and the recent stuff with these kind of weights, for example, or you can, you know, tune the weights with another model, for example, right? And you can express these types of queries uh purely in the in the embedding. Um such that like 

41:34
Host
with the, but how do you do it with a date with recency? Because like, let's say we have a model that embeds a document and somehow contains the recency information, but one month it will no longer be recent. Does it mean we will need to always recalculate the vectors for the alt? 

41:55
Guest
If you do it naively, then yes, you will. And uh so that's a bad idea, right? Um But there is a way to encode a time stamp uh into a couple of uh basically uh vector dimensions. Such that when you do a cosine similarity between two such and coded time stamps, it behaves like a normal time delta. Uh And you know, cosine similarity is basically like angle between vectors. Uh And there is like math uh which is by the way, a spoiler alert for the people listening. The math is somewhat similar to the uh to how the Pos uh Transformer model that's positional encoding. So when the Transformer model eats uh big string, right, uh the innovation there that is happening uh in parallel, right? It eats all the words in parallel instead of a sequence like LSD MS. Um But if it only ate like embeddings of all the words on the input in parallel, it would lose the sequence information, right? It would no longer uh kind of uh understand in which order the words came in. And the Transformer architecture solves this with a trick called positional encoding where basically you add into the same set of dimensions um information of the ordering. Uh And you know, this is like a little bit crazy because you add it into literally the same set of dimensions, which means you uh basically it's translation if you move you pret each word in the semantic space uh with some kind of delta and then the model, the next layers of the model disentangle this information somehow, right? Um But we do it uh using separate set of dimensions, right? We literally dimension wise concatenate all these signals into one big vector for you know, content and users and other entities. Um But yeah, this is this is the type of uh puzzles that uh you have to solve when you decide. OK, I want to express these complicated objectives and these complicated data objects purely in the vector form. Each new property type uh will generate uh this kind of puzzle or then you go completely custom like let's just make a custom embedding model. Let's feature engineer all these inputs. Uh And let's train a model that encodes all this data into uh embedding. And let's figure out how to constrain and train that model and you kind of go that way. 

44:36
Host
Yeah, interesting. So basically summary is that you can encode the time stamp in also the vector form and then the similarity between now and the time stamp in the past gives a sense of recency, right. And then you can also prioritize recent articles if it makes sense or prioritize relevancy, if it makes sense, right? And then the model would be smart enough to figure out like what is more because like I guess there will be one vector and then part, one part of this vector is residency, one part is relevancy, right? And then, 

45:09
Guest
and the the kind of key observation is to uh normalize all these components. So when you index any kind of data, you want to do this uh as di as free as possible, right? And, and this means that uh you wouldn't, you will not be recomputing the indexes match when you find your favorite biases, right? You want to postpone the decision of which signal matters, how match in which context as late as possible. It's the query time ideally, right? And so in our system, basically, when we uh embed these complicated uh entities, we normalize those components. And then when uh you use uh RSDK to formulate those queries, uh that's where you can start applying weights and that's where you can also start to train the weights, right? Um Because in different contexts, uh the weights will be different, right? Like your landing page is probably different than for you page uh in a category page. And um and obviously, this depends very much on the, on the use case as well 

46:17
Host
and which like speaking of this. Um So I'm, I'm thinking about GP T. So I know G BT S, they don't have this information about the time, right? So they don't like if you say uh you, you somehow need to, to be explicit, you say in your prompt, you say today is this day, right? Then you add a bunch of articles and then you say, OK, like I want to um when you answer my question, keep in mind that today is this day and the, the, the, the the things that the time stamps you have in the prompt at this, right? And then I can figure out that uh OK, this is my answer. So I know like for example, we haven't talked about that. Mm In data do club, we have a bot one of the community members, Alex. He created this chatbot for our courses to help students find the answers. So we have long fa Q documents which are very far hard to do to use to find things. So we have a bot that answers questions, right? And then in the prompt, what Alex is doing is it says today is this day and keep in mind when answering questions, right? And when somebody asks, uh can I still get enrolled in the course or something like that? And it knows uh that. So it's, I think it's similar to what is that? Right. Right. 

47:37
Guest
Right. So, um this kind of thought of basically stringify of time stamps and then eating them with the language movement um is within the broader packet of thoughts of, hey, let's stringify things and encode them with the LLM. Um This is limitations because the underlying model doesn't have like the exactly same understanding as you or me of how time stamps increment, right? So there can be like surprising uh results of like holes, for example, right, that or uh kind of misordering. Um So, yeah, I think the main problem of, yeah, let's just, you know, take this complicated entity. Let's a user with all their history, uh make big string and run it through a language model is that uh you lose the control, right? You don't get to say like what is, how much important? Um And you, you lose the kind of efficiency of using specialized models to encode subsets of the data because each that there are separate basically research fields in how do you turn graph structures into vectors or how do you turn time series into vectors uh or you know other types of data. So you know, they're kind of dedicated models for, for doing that for different data types. And if you try to do the whole thing with the LLM, it's uh computationally inefficient, hard to control. Um and the resulting retrieval quality won't be as good. Uh So I think those are the three main issues. 

49:19
Host
I see an interesting question from Demetrius. So Demetrius is asking if uh you have any publications that go into detail about the approaches you described on how to combine various signals into a single vector. 

49:35
Guest
Mhm. So we have a few um uh pieces uh that you can understand kind of between tutorial and like a research exploration on vector hub. So if you go to hub uh dot Super link.com and I think you'll also include the link in the in the notes. Uh We already have an article out there that uh illustrates how to combine graph and text embeddings together and also image and text, text embeddings. Um So, yeah, hopefully that can serve as an inspiration. Um 

50:11
Host
It's the name I found on the retrieval from image and text modalities. Is this the one? 

50:17
Guest
Yeah, there is that one and also another article that's worth checking out. It's called the representation learning on graphs or something like that. 

50:26
Host
Yeah, I can see representation learning on graph structure data and you see this in the navigation panel. Yeah, it's under the block section. 

50:40
Guest
Yeah. And and again, this is not the idea to, to take structured and unstructured data and put them into a vector is not new, right? Like uh in the big tech uh people have been building custom embedding models that combine structured and unstructured information into a shared uh vector representation for very long time, right? Uh The question is now more about how do we product is how do we let people quickly e experiment iterate? Um And you know, we are very close to launching uh our actual product, right? We have been in uh private and I don't want to be like too healthy. Um But basically we have a framework that's coming out that helps you do this, right? 

51:31
Host
And I'm looking at the article at the blog post, you mentioned this representation learning on graph structure data. So from what I see you show how to use pytorch. 

51:42
Guest
So in, in all of those examples, we we use uh standard tools out there so I could learn no pytorch. So right now, there is, you know, our goal in Vector Hub is to help people kind of learn the techniques and not uh you know, push our product uh that will be a separate place for uh kind of uh describing what our product does and how it relates to all of this. Uh But this, you will notice on Vector Hub it's uh kind of external contributor driven. Um And it's kind of uh uh compromise between entertaining the research, uh thoughts of uh practitioners out there and steering them towards. OK, let's look at kind of vectors and information retrieval. Um Yeah. So, so I think that's just a good place to start learning about the concepts. 

52:35
Host
And then I see that you also have vector DB comparison, which is a super relevant thing because like if you Google or if you just open any article about LLM or also take our interviews, I found that there are so many different uh vector databases. And from what I understood this uh this article that I have here or not article this comparison database HM actually helps us to pick the right database for our use case, right? 

53:09
Guest
That's right. So we crowdsourced uh this uh it's, you know, it's powered by uh Gin repository, the sa same as vector hub. Uh And we crowdsource this uh and also we got a bunch of contributions from the vendors. Uh and it's basically a compa feature comparison of vector databases, um you know, for different types of search constraints and operational questions. You know, can I run this in the process with my app? Uh and separate? And is there a managed offering? What is the open source license? We also have stats now of G upstairs and you know, M PM pools and uh pin styles and, and all of that stuff. Um and look like people sort of look at the table and they think, OK, this is like way too many offerings in the market. Um But actually, I think, I think that uh we haven't really uh you know, the the full potential uh that this technology enables. And I think as we go and apply the technology, there will be a bunch of different specializations and different buckets where different solutions perform better. Like do you want few big indices versus many small ones? This alone is, you know, one of those decisions that make that inspire completely different design of the underlying system. So I believe there will be a bunch of these things and obviously the the incumbent databases, they all basically lo the vector index as well and there are different tradeoffs of, of that, of course. Um But yeah, I think that table. So VD Bs dot Super link.com is a good uh starting point for that exploration. I would say 

54:56
Host
I see that we have another interesting question. From Vaka. So the question is, is there any reason why I wouldn't use, you wouldn't use a database that goes beyond just vector search. And then I immediately thinking about databases like elastic search or Lucine, right, where we can actually combine like we have this uh mas and shoot type of queries, we have the the inverted index like the word index, then we have a bunch of other things, right? And then also in elastic search, I don't remember if you still need to install a plug. And I think like now it's um uh it comes in with Lucine. So then you organically have vector search and all loin based databases, then you can just use vector search in your database. So like why can't we just put everything in lucine and like let it do its magic. 

55:51
Guest
Um You might maybe that's a good place to start, right? Because it's right there. Uh The, the question, I think the the considerations uh break into a few different categories um for a while it used to be performance like OK, if you have couple million vectors, uh the PG vector and other solutions were uh considered orders of magnitude slower than the dedicated solutions, right? Um I think there is still some difference, people should uh check out if the difference is big enough for them that it matters. Um So performance, the other category of thought is what is the right set of tools and abstractions around this new type of search, right? For example, query language um to what extent you can tap into, right? Uh I mean, people had similar, yeah, so people had similar thoughts with uh graph databases, right? Um I, you know that that's I think we have some successes and some challenges to, to learn um uh from, from, from that era as well, right? Um I would say the question is what do you are optimizing for? Are you optimizing for only ever having one database or are you optimizing for? So, you know, solving business problems and building stuff that works as fast as possible. Um Then, you know, if the new abstraction helps you deliver faster, it still gives you the expressive power and need, but also uh gets you to the destination faster, then uh maybe you should look at the tool, right? Um Yeah, I think that's kind of the decision space. 

57:40
Host
Do you have uh more time or do you need to go? Uh 

57:43
Guest
Yeah, we can, we can uh keep going for a couple more minutes. Yeah. 

57:48
Host
Well, maybe this is a longer, this will require longer time. So a question from AJ is if I'm a mid size D to C brand, direct to consumer, I think uh brand, what would be the best way to build my search tech? I'm looking only to add personalization which from pricey third party vendors. 

58:11
Guest
OK. That's a, that's a big one. They probably need a consultation. Right. Yeah, I mean, very happy to also, you know, for, for, for any questions that remain unanswered. Uh I think there'll be a link to my linkedin. People should connect to me and, and, and show those uh questions over, um, for the, for ecommerce. I think there is a huge opportunity to, um, do real time personalization across many different surfaces, feeds, category pages, product detail, pages, basket, uh, pages, personalized emails type. Um In fact, our first kind of production deployment is of this type and we see uh um large lists of, of uh revenue and so on. So I think there is a big opportunity there. Um Also because of the multi modality of the ecommerce data, I do often have product images and descriptions and behavioral data. Um I think there isn't, you know, like go to stack uh that uh you should absolutely use. Um Yeah, I think it depends how much on the constraints it look like. If you have uh let's say um 100,000 data points across all your staff, right? Behavioral events and products and users and everything is kind of on that order of 100,000 couple of hundreds of thousands. Then I would just uh pull the data into a Python notebook and just kind of see what you can do with basic tools out there, right? Do some embeddings, do some matching, pull uh frequent queries that you get on search um see if you can make embeddings of users and cluster them to see if um you know, there there is uh there are some clusters to be exploited. Uh I think you can explore quite a lot in this way. And then if you are getting results that are dramatically different from what your current production system is uh doing, right? Like this, you can literally just eyeball, right? Like for example, the clip model that you mentioned from open A I I think this is like eye opener for many people that they can ingest a bunch of photos of clothes and then they get the search credit like blue T shirt with short sleeves and it actually works. Uh And it differentiates between short sleeves and long sleeves like this feels kind of magical. Um So I would, yeah, like most people start this way, right? They kind of create a demo of some queries that uh are dramatically better than the current system. Um And then they figure out how to product is that right? Probably some kind of titan server. Um You know, getting all this data on the input, handling those queries, there's probably a vector database, uh or vector enabled traditional database somewhere in there. Um I think that's a cool, cool place to start. Maybe, maybe we can do one more and then I'll have to them. 

61:24
Host
Yeah. Well, um so we have other questions. I know they are not uh I don't know how they are. So this one is also big. So the question is like, what are some metrics that can be used to monitor search performance? 

61:41
Guest
Yeah, I mean, that's a, that's a huge one because performance is very ambiguous, right? Um Actually our chief architect likes to say uh that the main metric that should be used is uh USB and I and I love this joke because people, people exactly because people uh you mean square error, you know, they, they try to figure out like what uh what abbreviation is that of USB. But yes, it's dollars in the end, right? Um So high level thought on this very long question is you'll get more funding for your project uh as a data scientist, as an engineer in your company, if you can connect your metrics to the actual business performance. Um And then uh yeah, do A B testing um carefully and uh intentionally. Um and uh I mean, there is, there is so much content about this out there that I don't think I can, you know, do it justice. Um But yeah, I think the, the dollars uh I think that's my kind of, that's the delta to most of the content that I see out there is uh connected to something that the business cares about, not have 50 Grafana charts that uh only you care about. 

63:05
Host
Mhm Yeah. So like sometimes it's not immediately possible to, to calculate the impact in dollars, but sometimes you have some, you can have some other business methods that is important too. So for example, in the company where I used to work, we cared about contacts. So it's it was like a marketplace. And what we wanted is from search if somebody is looking for something, then they contact the sellers, right? So this is one of the important things or click at a certain thing like uh order a delivery, right? So these are two metrics that are important. And then for each of these successful events, we can attribute some money value, right? And 

63:49
Guest
those are the proxies, proxies for the dollars, right? Like that's the only reason that you would care about somebody contacting a seller is that somebody figured out that there is some probability of that leading to uh transaction down the line, right? And then you think about the panel and those probabilities and kind of all things being equal, more clicks probably means more money. Uh Usually the all things being equal, there's a lot of heavy lifting because we have experiments that are not fully isolated and all kinds of seasonal effects that upset uh this uh equalness, right? That's why we run uh control groups. Um Yeah, the the yeah, like search, search element uh monitoring uh is, is definitely uh um may maybe one more thing I'll say on this topic is that having metrics that uh engineers can a f without going through the data scientist um and iterate on them quickly, I think that's interesting. So um basically how can you create metrics that facilitate fast iteration and sometimes that could be offline evaluation tests, right? Sometimes it can be a B test. Uh But one of my kind of goals uh or our goals with super link is to enable the engineers to solve a lot of these challenges, right? Without going through the data scientist, because the data scientist is busy has many problems should work on them. Um But for the some of the more basic stuff or clear stuff, um we want to give engineers the levers to explore and uh and uh still have the the power but also uh the the kind of uh abstraction that helps them actually navigate the problem and it feels more like engineering and less like uh magic, right? Um I think with the pre trained models, this is one way to understand the current opportunity in the, you know, current sort of uh Ml hive wave, right? Is that engineers can solve information retrieval problems directly. Um Th this this I think will unlock a lot of value. 

66:19
Host
Yes, certainly we didn't talk about uh the algorithms and the competitive programming and the relevance to everyday work maybe some other time. And I see actually by the way I see right behind your head, I see a bluish patch of sky. 

66:37
Guest
Yeah, that's uh OK. I I'll play with your optimism and say that I also, uh but I'm tuning in from London today and it's the beginning of March. So, 

66:48
Host
yeah, I was going to say maybe you can now go celebrate this, uh you know, blue sky maybe. OK. Yeah, thanks a lot for joining us today. Thanks everyone too, for joining us today and for asking your questions, tuning in. And also thanks Super linkedin Doctor Hub for supporting this podcast interview. 

67:10
Guest
Thank you, Alex, a uh big fan of the community of the podcast. I actually binged a few episodes uh just recently. Uh Please keep doing what you're doing. Always good to have this kind of engineer first view, you know. Um And uh hopefully we get to chat soon. 

67:28
Host
Yeah.