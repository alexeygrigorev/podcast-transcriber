{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24939e92-304a-406b-a08c-d178bbd8c4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46597b1c-d8aa-4ca8-9fdc-f389960a4337",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('podcast-test.json', 'rt') as f_in:\n",
    "    transcript_job = json.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5be4a6e3-c0b7-4bf0-a30a-306708cc25ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['transcripts', 'speaker_labels', 'items'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = transcript_job['results']\n",
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89f76ace-c2d6-4068-a09b-5a7b97fe381a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results['transcripts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7113b478-77e7-48cd-b879-444660e7c242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12009"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results['items'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "348adc14-eb2e-4b06-8168-3e65cc31eba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_timestamp(seconds):\n",
    "    minutes = int(seconds // 60)\n",
    "    seconds = int(seconds % 60)\n",
    "    return f\"{minutes}:{seconds:02d}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "87a90272-c068-4efe-bfb4-4335616dfbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_text = \"\"\n",
    "current_speaker = None\n",
    "current_start_time = None\n",
    "timestamp = '0:00'\n",
    "\n",
    "names = {\n",
    "    'spk_0': 'Host',\n",
    "    'spk_1': 'Guest'\n",
    "}\n",
    "\n",
    "for item in results['items']:\n",
    "    # Check if the item has a speaker label\n",
    "    if 'speaker_label' in item:\n",
    "        speaker = item['speaker_label']\n",
    "\n",
    "    if 'start_time' in item:\n",
    "        start_time = float(item['start_time'])\n",
    "        timestamp = format_timestamp(start_time)\n",
    "\n",
    "    if speaker != current_speaker:\n",
    "        current_speaker = speaker\n",
    "        current_start_time = start_time\n",
    "        speaker_name = names[speaker]\n",
    "        transcript_text += f\"\\n\\n{timestamp}\\n{speaker_name}\\n\"\n",
    "    \n",
    "    if item['type'] == 'pronunciation':\n",
    "        word = item['alternatives'][0]['content']\n",
    "        transcript_text += word + ' '\n",
    "    elif item['type'] == 'punctuation':\n",
    "        # break\n",
    "        punctuation = item['alternatives'][0]['content']\n",
    "        transcript_text = transcript_text.rstrip() + punctuation + ' '\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1e521db4-1252-4166-b94d-c9a841caa318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'punctuation',\n",
       " 'alternatives': [{'confidence': '0.0', 'content': '.'}],\n",
       " 'speaker_label': 'spk_0'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "97615d0f-45da-4056-9feb-2c598a5a970e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0:00\n",
      "Host\n",
      "Hi, everyone. Welcome to our event. This event is brought to you by Data Dogs Club, which is a community of people who have data. We have weekly events and today is one of such events. If you want to find out more about the events we have, there is a link in the description, go there, check it out, you'll see other things we have in our pipeline. It's been a while since we had a podcast interview, like the one we have today, we are working on getting more guests to our podcast. So you will see more and more things uh there. So do not forget to check this link and also do not forget to subscribe to our youtube channel. Also, we have an amazing Slack community where you can hang out with other data enthusiasts. So check it out too. And during today's interview, you can ask any question you want. There is a pinned link in the live chat, click on that link, ask your question and we will be covering these questions during the interview. So no, I will open the document with questions. OK? And we can start if you're ready. \n",
      "\n",
      "1:13\n",
      "Guest\n",
      "Yeah. Yeah. OK. \n",
      "\n",
      "1:16\n",
      "Host\n",
      "This week we'll talk about how is it to work as an open source developer and core developer in the C Learn Universe. And we have a very special guest today, Yong Yong is an open source engineer at Probable and he's focusing on uh data science development and contributing to the Clan Library. He's actually not our first guest from the pro um company. Uh We recently had an interview with Vincent and uh yeah, so very excited about this. Of course. So Guillaume is also an open source engineer at Tre and Tre for those who don't know which is the organization behind psychic Learn has been for many years until pro appeared. And he worked there for more than seven years. He also has a phd in medical imaging focusing on computer aid diagnostics from for prostate cancer. So welcome to our show. \n",
      "\n",
      "2:11\n",
      "Guest\n",
      "Hello, happy to be here. \n",
      "\n",
      "2:14\n",
      "Host\n",
      "So the questions for today's interview are prepared by Johanna Beer. Thanks, Johanna as always for your help. So let's start before we go into our main topic of being an open source engineer and work working on psychic learn. Let's start with the ground. Can you tell us about your career journey so far? \n",
      "\n",
      "2:32\n",
      "Guest\n",
      "Yeah. Uh I can do quickly these things. So pray I will not start with master whatsoever. So I will start with my in my phd. So basically I was a user of psych. I mean, before I was using basically math lab and I was doing uh basically I did a phd in, in uh like detecting in uh medical imaging, I mean MRI imaging and then you have to do those on the cluster. And then the licensing of my lab just block you because you have 300 nodes and you have 10 licensing and that, that is on scale. So, I mean, we don't tell you to do that. So this is where I started to use like Cyle. And then I was so 2009, I think something like that or 2011, 2nd \n",
      "\n",
      "3:17\n",
      "Host\n",
      "second learn was already, I think \n",
      "\n",
      "3:20\n",
      "Guest\n",
      "it was already a thing. And they are like, why I was really interesting about it was they had this random forest classifier that was working very well for my use case. Um And then I needed as well. This, I had this frame of, of data balancing because you have much more uh like books sell without any cancer, why it work? Excel with cancer on those images. So I had like uh imbalance programs and it's why nothing was there. And then by reading the literature, I said, OK, let's let's make something that is compatible. And then we just hanged out with some people that uh were, were as well interesting about that things. Um And one day I went to, to see a seminar where I saw Raguel Baco and say, oh by the way, I mean, how is it going at, you know, is there any job and whatsoever? And, and one just present like a few days afterwards and how I landed, \n",
      "\n",
      "4:12\n",
      "Host\n",
      "this is the guy behind second, right. \n",
      "\n",
      "4:15\n",
      "Guest\n",
      "The \n",
      "\n",
      "4:15\n",
      "Host\n",
      "one \n",
      "\n",
      "4:16\n",
      "Guest\n",
      "is the co co founders um of, of Psychic Learn. Basically when, when they started a project like uh 2011, I think, something like this and, and by knowing that I was working on related stuff, say, oh, so you are pretty a good candidate to speak with because we want to maintain as well like the ecosystem and, and having people that are like, have a tendency to touch those things. So come and interview and then we'll figure out something and then I never left. OK. So I ended up in at, in area the modality of how it's like this evolved over times uh first like as an engineer within, in a yard and within the foundations. And then now probably just like a continuum of changing and trying to make sustainable, like pay basically engineers uh for, for these open source packages. This, this is a bit like the track. \n",
      "\n",
      "5:08\n",
      "Host\n",
      "Uh So I only learned about psychic learn in 2013, I think maybe 14. Uh Yeah. So like all the courses I took was about either R or octave. And like for me, I still didn't understand that Python is actually quite convenient until I think I saw a course about that and then found psychic learn. And this like I it was really good interface at the API reason. So I think OK, I really like the way it's done, it's so clean. Like with all this feed, predict turn down ever since I'm in love with this library, \n",
      "\n",
      "5:51\n",
      "Guest\n",
      "it was something similar because I started the phd in 2011. But I did like at least half of the phd in MATLAB. So 2013 or 14 was pretty like the place that say OK, actually this like this language and then I never touched and it seems like something much, much easier to use. So let's go there. \n",
      "\n",
      "6:10\n",
      "Host\n",
      "Yeah. So back then I actually like in my main activity, I was a software engineer and I was a Java developer and back then what people used for Java was Becca. I don't know. Have you heard about this thing? Which one we a so it's a, I think it's from the University of New Zealand, somewhere from that part. \n",
      "\n",
      "6:33\n",
      "Guest\n",
      "Yeah, because it's a QE that is on the page. So, yeah. And yet at the time, I think that you had a wrapper, a ma a wrapper just to when I go and call the Java implementation. So I, I kind of like use it at some point as well. Um Yeah, it was not as easy as just like turning into Python and doing, doing things there. \n",
      "\n",
      "6:54\n",
      "Host\n",
      "Yeah. So this is how we all ended up in uh in the Python world. Yes. And you've, so you've been working first as a user, uh you've been using secular as a user first and then you became an open source developer, right? And you've been doing this for 7.5 years, roughly, right? \n",
      "\n",
      "7:14\n",
      "Guest\n",
      "Yeah. Yeah. Something like this. Um Yeah. First as the, I mean, when I was developer, I was using psychic and then I, I somehow like developed this compatible library. So I knew a bit like some internals. But roughly, and my journey really started when, when I collaborate or work with people, basically, I, you know, yeah, because then like the insiders open and then uh people explain you like more stuff \n",
      "\n",
      "7:39\n",
      "Host\n",
      "and this uh compatible I think you said compatible libraries. So one of them is imbalanced, learn, I think, \n",
      "\n",
      "7:46\n",
      "Guest\n",
      "right? \n",
      "\n",
      "7:48\n",
      "Host\n",
      "And others. \n",
      "\n",
      "7:50\n",
      "Guest\n",
      "So yeah, right now we are working on something that is much, much recent which is called scrub. Uh and this is to scrub basically your data. Uh and, and bringing back, let's say your data sources that could be sul table or any type of sources, bring them like closer to your machine learning models because secondary is very good ones that you have your T data X and Y and then you want to pre them slightly and put them inside the machine learning models. But you cannot do like extreme or like joining and whatever pandas is good at. But it doesn't have this, this like state thing. So scrub is more like inside this, this uh like scope of like bringing close as a data source to the machine learning one. So that's, that's like some tool that we want to make them like compatible and, and, and imply inside the development of those two tools. \n",
      "\n",
      "8:44\n",
      "Host\n",
      "Mhm. I'm looking at the get up, read me. So it's a scrap formula. Dirty Cat. It's a Python library that facilitates prepping your tables for machinery. \n",
      "\n",
      "8:57\n",
      "Guest\n",
      "Exactly. Because so the original idea, so when it was called Dirty Cats, it's linked to a research project from Gael Maco actually, where it was interesting to say, instead of trying to just take a columns and, and have to clean manually everything like for instance, categories which are not uh properly formatted or there's typo and stuff like that, let's stop to do that. And let's come with a machine learning approach where you will just reduce the amount of time that you have to do this uh manual preprocessing and come with uh as good preprocessing with like your statistical approach. So maybe I bing maybe like over type of, of, of things that go on. And uh and that was the original idea. And then it was maybe we can just like a larger bit of scopes, not only categories, but as well to if I had date times where I want to have like uh like on the shelves, encoding of those uh the same with uh high cate cardin category low, low cardin categories and, and numbers. I mean, what can we do there? So that, that, that you just of good. Uh \n",
      "\n",
      "10:06\n",
      "Host\n",
      "And the cat here is not a, a cat and animal, it's a category, right? \n",
      "\n",
      "10:12\n",
      "Guest\n",
      "Yes, exactly. Yeah. So yeah, dirty cats who has like yeah, dirty categories basically at that at the time and the was a dirty cat but a real dirt. \n",
      "\n",
      "10:23\n",
      "Host\n",
      "So I this is something as a data scientist I encountered quite often that um you have a column which is ceg call, but you have so many like for example, country name. And if you let users type country instead of giving them a list of countries, then you end up with like all sorts of things like us us usa with dots without dots uh the US like all sorts of things, right? And you want to normalize it somehow and this is what the dirty cat would help with, right? Helps with. \n",
      "\n",
      "10:55\n",
      "Guest\n",
      "Exactly. So yeah, so this if, if you think of it like, I mean an M LP approach. So like with our network, you can just project those into an light and space like an embedding and all those us and variation of it should be close together. So I can just like provide basically a vector where all of those would be like close to, to to each other and like something that is completely different will be far away. And I don't need to treat like the original data in some way. So that's uh that's uh I could maybe cross the inside an orbiting space and just like trying to repro bikes, let's say, but uh that's uh let's say the projection is good enough to start to do machine learning. But this is exactly. Yeah, that, that type of processing. \n",
      "\n",
      "11:39\n",
      "Host\n",
      "And uh yeah, so you've been working on psychic learn for quite some time. Did you focus on any specific part of psychic learn or you worked on the entire library or what, what did you work on? \n",
      "\n",
      "11:52\n",
      "Guest\n",
      "Um So now it, it's quite a while. So I would say now I have an overview of what's happening, but of course, depending on the time, I'm just focusing more on something or, or something else, one thing that I really like, so I'm not a performance person, so I'm not doing low level things I can review that but I mean, it's not, I'm getting annoyed after sometimes. Um I'm not like a pure math approach. Like I don't do like really like low level maths or like really going to the bottom and optimization. I mean, this is not my, my like where I am the I can have the most values and somehow I, I end up in, in looking more like higher level API things. So for instance, I'm I'm really interesting at seeing what type of value we can give to users when they want, for instance to plot evaluation metrics or those type of things. And then you need to have a good IP I for that and you have the model, then you want to take it there and make like nice plots or sometimes as well on, on business cases. So let's say, I pray we will come back later on that. But like I can have this for a very long time. Uh The predict method just can take the probability in in the classification cases. And to make the final decision you get at 0.5 which is very confusing when you enter in inside that nobody really question that, that things and and if you are inside a business grade, that thing is not the right one. And then, so for instance, we recently recently developed this, this meta estimate which allow you to retune this depending of, of a business metric. Uh So it, it's more on those like we, we see over time some very important things inside the road map. And I I will take like usually what nothing that is really related to low level maps or low level performance and more to API and sometimes going to like a very deep into the application and what can bring values to users. \n",
      "\n",
      "13:54\n",
      "Host\n",
      "OK. And like when I ask this question, like what part of psyche learn you worked on? I but I realized that I don't actually know like what parts are there. Like I know that there are different sub modules, like three sub modules, the ensemble sub module, the linear sub module. How does it look under the hood? Do you have like how do you like if you wanted to split second, learn into multiple areas, multiple parts, how would you do this based on modules or based on some something internal or how does it look like? \n",
      "\n",
      "14:29\n",
      "Guest\n",
      "So right now how it splits is really in in yeah, some modules and family of of algorithm. So you have the preprocessing with like a bunch of things there. But then as you say, like if I have models which are like linear models, they will be inside that modules and some will be like as well in in their own modules um and trees in their modules if I have nowadays to to define uh what organization makes sense to me, it would be uh it would be like to define the scope basically where secular is good and where maybe it should move a bit would be we should have numerical algorithm. So this is three linear models are like really you put an umpire right inside and is what was from the being you put an umpire right inside. It does some mathematical operations to give you output. And uh and at some point we as well add like preprocessor for instance, scaling and all of this. And I assume that nowadays there is a kind of a value and maybe scrubs can shine because there's a value to have like different tools. For instance, polar bring like lazy data frame pond that was as well a data frame and it's not anymore an umpire way that you want to put inside us. It's much more than that. It's much more like heterogeneous and have more values. And I think that nowadays, I will just like split into like component one which is like the really applied math where you need like efficient algorithm with solvers and all of those there. And then the other part which is inside the preprocessing and pretty like we should have another vision on, on what those tools are doing. But I mean, they should help you to put them then inside the miracle part. \n",
      "\n",
      "16:15\n",
      "Host\n",
      "Mhm But should it still be a part of psychic learn or like in a separate library? Like a scrub? \n",
      "\n",
      "16:21\n",
      "Guest\n",
      "That's a good question. So today they are inside A K uh and we have for instance, difficulties to be able to be compatible with pandas pra and all of those, what scrap could be in the meantime is a playground to see what we should support and how to support it very well. And if at some point we have like something uh that you can just instead to use the solar scalar from, from C and you have a version in scrub that exactly do but like speak to different back then. Probably it makes sense to say, you know what C will be the fundamental, I mean, like the foundation models of, of like uh T data and then like, we should restrain maybe to this, I have no idea. It's not a question, a discussion that we got in the rat because I mean, it's very early but uh that might make uh things more flexible actually in the, in that regard. So we should not stop ourselves to say maybe we should like remove some of the part because nowadays, with all the back end that we have and to make it life easy to our users, we should like have this distinction and maybe scrub will become like a cycling thing. But I think that preprocessing should be split maybe from the numerical part so that people don't come and say I want to be able to put whatever like metric inside my trees because at the end this is a numerical operation. So we will have to convert into NPI or KUPPI or to an array because this way it will be very efficient. \n",
      "\n",
      "17:52\n",
      "Host\n",
      "Mhm Yeah. And also since I learn is uh I would say the most popular popular machine machine learning library in the world. It's not something you can just say. OK. Yeah, we have this preprocessing model but it doesn't really fit there. Let's remove it because so many people depend on this, right. So like I guess for you a big challenge is like, how do you change this thing? Because like, uh, the moment you change something in this library so many, like, you have this, uh, large domino effect. Right. \n",
      "\n",
      "18:27\n",
      "Guest\n",
      "Yeah, I, I will take a typical exa, I mean, like, uh, the strongest example that everybody in the Python committee pre, like, observe if you are at least maybe 20 years old. Uh, Python to Python three and what, you don't know where, where, I mean, now, I know because I'm 35. Like if you're 20 you don't know about it because I mean, it's already Python three now, but Python two should have been a change in a couple of years. I know two or three years it lasted, I don't know, maybe 10 or because uh like, yeah, it was like some difficult part and make people switch is super difficult. And since that is so costly, you really question if this is worse or not. So here we don't want, I, I mean, we have the same vibes in, in, in cyclone is it is we don't want to break your ecosystem much better way to do. Uh So we need to have these very long transitions. And for instance, we, if this is like coming from like really splitting estimators, that would be a very rather big change. And then we need to understand how smoothly we can do because moving and imports, it's already breaking a lot of things. So uh uh changing behaviors may be even worse because like changing behavior people get surprised and then they say, oh, I need to have this V one to V two and now everything needs to change. So this is like, uh, so it's really something that I would like that we don't have moving the import is maybe something that, uh, is like, at least it doesn't change the behavior. So that might be ok. But, I mean, it's not something that you just say, ok, now it's secular one. So now is it secular two and then you change stuff? Uh, so it, it didn't need to be long and up to now we always have been like, uh, throwing down everywhere because of those making it like, uh, a very trust for like labor in the way that we don't break that. \n",
      "\n",
      "20:27\n",
      "Host\n",
      "Mhm. And also, um, like when you make a change, I, I get a warning and this warning stay for a few major releases, like, until I'm ok, finally I'll fix it. Just stop doing this, stop making this warning. \n",
      "\n",
      "20:44\n",
      "Guest\n",
      "Yeah. So it's exactly that. It's one year we, we give a one year, uh, lapse of time that people can. So six months where we change and then we start to change ourselves, the code and the release will be six months later. So I give you like one lap of one year to add up to the code. Uh And I assume those people just like, maybe just annoying and just like pin the versions. I mean, that's, that's how it can end up. I mean, in companion because you don't want to say anything. So that, yeah, it even doing this is like already the challenge in some places. So we have to be aware of this. \n",
      "\n",
      "21:19\n",
      "Host\n",
      "Well, people still use Python too for some things. Right. So they pin versions for a second loan. \n",
      "\n",
      "21:28\n",
      "Guest\n",
      "Yes, that's true. Uh At least now the PP I uh repository tell us that a lot of people just moved to Python three. It's, it's, it's much nicer than what it was. Still people using uh the 1.0 versions and we add 1.5. So maybe it's only CIS that's like try the minimum version that they support and then uh we get like a lot of downloads there. But this is kind of interesting to, to look at like that numbers, for instance, collab is using cyclon 1.2 0.2 I think. So it's like you teach people using like as well all versions. So if today you say we don't care, we don't support anymore the old versions or like we don't give uh like any interest, like in the next six months, we break everything then I mean, yeah. No, no, not that much. People will be happy about it. \n",
      "\n",
      "22:19\n",
      "Host\n",
      "Um Was I affected with the recent NP 0.2 release 2.0 \n",
      "\n",
      "22:26\n",
      "Guest\n",
      "that was a very interesting uh community effort. So yes, we got uh affected uh in our development. But I assume nobody as a user would have felt it because we, so what happened is that uh now with the PS IP community, like uh scientific Python community, these have been very well organizing. Oh we are going to do this and it's a long time that basically we are testing nightly build. So we are testing already since maybe a year against like Nam P two. And then we discussed as well with the developer in different comprehensive points and your I with people from N PC, we will make this A B I. So you probably want to make releases in between. So that, I mean, we have a transition that people don't uh like from day one to day two, they just like install something and it's not compatible anymore. And I have found that it went very, very somthing. At least we have the chance as being secular to only depend on non pi I pi. So that's easy for us. But at least we didn't block anybody that's uh depend on secular because we are already like ahead of that. And if the metro package just like Pandas did the same uh and over like major libraries did the same. And I assume then it's up to the people that are dependent on those libraries to make the move. And when it comes to Konda, I think that K A four is just amazing because they can just like if people take care, then they can just rebuild all the packages. So it went very, very smoothly but because of the community efforts as well, I mean, and communications and that's very interesting things to me. I mean, for, for me \n",
      "\n",
      "24:11\n",
      "Host\n",
      "and you're also quite tightly connected with each other. I mean uh Namai Sais learn cause like, I guess they know that you're one of the major users. So they want to inform you about all the breaking changes. \n",
      "\n",
      "24:27\n",
      "Guest\n",
      "Yes. So the, the somehow is like if they have a measure changes, they want to say, hey, by the way, on your side, how bad this is. And then it was the communications and uh and yes, the identity pipe ecosystem relies on a lot of people that are the same everywhere. So you have uh for instance, quanta that pay opens those developers for instance, like H goers and you can see R goers for instance in Namai in Sai and then we know who to ping. So in second and we say, oh we have that question about what's happening and then it will be directly like answering that thing here and then in Cona for that as well, some other people that are really like tight. And so it's something which is scary in one way because actually the community is not that big compared to the impact of what it does. But then it's very interesting because we always speak to the same people. So uh we need to em board more people as well. But, I mean, like this is very interesting that uh everybody know each other and are aware that what you should break and what you cannot break and how, how to make it smooth. \n",
      "\n",
      "25:35\n",
      "Host\n",
      "Mhm. And I guess you usually meet at, by date or similar conferences, right. \n",
      "\n",
      "25:41\n",
      "Guest\n",
      "Yes. So, um this uh one community in the US which usually go a in Pyon because now we have the data track and in Cai and then in, in Europe, we have uh Euros which is like a very small but very dense in, in co contributors uh and usually happen in August and otherwise it's like multiple pi data events. Uh and we see everybody in different locations. So for instance, Python D with pi data burning this year or like the year before it is one com one event where we see people in Python Italy is one as well. Uh And then this year will be uh PDA Paris and uh pi data Amsterdam. I mean, there are the thing that's just like there is many of them. So you are always be sure that if you go to one of those events, you will see some people. \n",
      "\n",
      "26:32\n",
      "Host\n",
      "Mhm Right. And speaking of pi data, Pyon Berlin. So this is actually where I saw your name and your talk and I thought it would be nice to invite you for this interview. Um And the talk that you gave was a retrieval augmented generation system to create the psychic learn documentation, right? Can you tell us a little bit more what you talked about there? \n",
      "\n",
      "26:57\n",
      "Guest\n",
      "Yes. So yeah, that's w some LLM within second and what's happening there. So, yeah, so, so the reason is uh we were revamping the website which is actually now like uh nicely done uh using this pet terms things. And uh during this revamping one thing that we know that is not like great is the search bar somehow that's using like the things. I mean, it's very minimum. It works if you know what you are searching for. But if you don't know, then this is not that great. I mean, it doesn't do fuzzy matching and all of those like of fuzzy querying. Um So it's a bit limited and we thought like maybe uh like everybody speak about those rag and LLM but none of us really knows. So that would be nice to make a book and learn first like the techno, what is it unlocking? And with our perspective of like open source package, what are the limitations? Because we have some and uh and if it brings some values and cause things so basically, to go fast in the result of this is uh limitation that we have when we are open so that we cannot collect data from users because we don't do that. We don't want, we could, we could ask people, do you want to give us like which search you are doing and it's to improve basically u usability. The problem is that if one day we leak those, I mean, we don't want us to take the risk. So we don't do that. FEV could be L GP D compliance. So we have no data at the plant. So we have no very way of evaluating. Uh And this is a big, big, big issues when you want to develop such like technology that you don't know what would be the impact and that uh you don't know basically the framing because everybody like it and, and evaluate it in what the thing that their user are using it for. And then there's all the thing that user will actually use it for and then on those things, you have no idea what it gives. And, and uh so this is a very like uh let's say a very difficult question, where is it the tool worth to put, if actually nobody will use it or use it badly. Uh So that, that was one of the thing there and then uh about the technique component itself. Uh It was just to know where what everybody tells you, right? Are simple, like just change your data, check your documentation, put it there and then it will work. And then when you do it yourself, you just like see a lot of issues. So my point was to say, OK, let's make something that is repo and let's look at what are like the, actually the P CS that people don't tell you in blog posts that you can read in five minutes. So le let's look at those and, and make a presentation and bring back that first vision of a committee and to go on all the drawbacks that could, you didn't think it at the vein and that could be helpful if you were thinking to start with that. So that's, that was the talking by Pyon D. Um And probably I will just like make a blog post with Probable or Rhoda topics. And now we have as well, this uh repository where you can just take you back and just like repo this that and have you like your right if you want to test it, uh If you really want to see like what, what these things gives. Um So that's, yeah, that was a bit like the idea of these things. \n",
      "\n",
      "30:13\n",
      "Host\n",
      "It's called Rugger Duck. \n",
      "\n",
      "30:16\n",
      "Guest\n",
      "Yeah. Yeah, I, I'm, I'm only proud about the name. So it's a ragged duck. So it's as bad as a rubber duck. Uh You cannot expect anything from it. Uh But you can answer some stuff. So \n",
      "\n",
      "30:28\n",
      "Host\n",
      "OK, that's a, that's another name. Uh And then speaking of uh second term documentation for me, this is one of the best examples to me of how recommendation should look like for a library because like it's so well documented and it's just like, and it has been like since I started using it. So when I be I became a data science full time in 2015, right? And then it was already so good and it keeps getting better. This is amazing. And but, but to be honest, I never used this search functionality on the website. I just usually use Google. \n",
      "\n",
      "31:08\n",
      "Guest\n",
      "Yeah, that, that, that's the thing is first is like if the search is bad, then you try once and then it doesn't answer you need, then you don't do it. So you have that, I never really use it. Yeah, so II I never really use it as well, but it was like, maybe we can do instead of that, maybe a chatbot would be nice. Um So most probably no. Uh one thing that we need actually is like something that is a good information retriever system that could be cheap. Uh And uh for instance, Algo of those type of things and then that's really like, let's say one improvement in the user uh like interaction experience for people that are using this bar for people that don't use it. Maybe then if, because if it's better, then they can use that. So we are much more thinking about like doing these things uh before to do like any right things. Uh The right thing is maybe we have to see because serving it as well, you need GP U and we are a community. How do we pay, pay for that. Uh So that's uh \n",
      "\n",
      "32:12\n",
      "Host\n",
      "M 777 B, right? This is what he used. \n",
      "\n",
      "32:17\n",
      "Guest\n",
      "Yeah. But, but the model, so you need to serve it. So somehow I, I need to get a server somewhere, loads on the GPO D thing and then I need to pay 75 cents an hour for that machine. So, and at the end of the month depending and we have 1 million users um that look at the documentation every month. If everybody start to just like make a request uh queries, how much does it scale? Uh So do you need more GP us? How much does it cost you? And I mean, we are not a company that can ask money to people and just like cover the, the cost. I mean, that could be one thing that we could do if you're a company, I don't say that regular useless for, for companies far from that is that that as a community perspective, we have much more constraints and then this is like, yeah, you should ask you sell those things and regarding the documentation, I really agree that we spend an extensive amount of time to put ourselves to non technical perspective, to write that documentations bring as well the the technical part if you want to go further. And the funny part is that I think that in the contributors, yeah, we like it but we see a lot of things that we just dislike and it may be why over time for people from outside it looks like, ah, it's improving because we hate some of the parts so much. Like you go the example and there's too many things and we say we should come and just like, like, pure ideas because that's not helping people. So maybe you're right would be good here to say, find me like the example that I search, I mean, that's uh we never know. But uh this is always like you can always do better. \n",
      "\n",
      "33:54\n",
      "Host\n",
      "Mhm And also documentation is one of the aspects where for us uh users of psychic learn is we can actually contribute easily. And these contributions are accepted because I myself have an experience of uh improving some examples. And then this uh pr was accepted like within one day or something like that. So it was pretty fast compared to other open source projects where there is like usually a long discussion and then a lot of um revisions. So that was just uh that was a minor fix, but I was so proud of myself. Um uh I can tell you \n",
      "\n",
      "34:34\n",
      "Guest\n",
      "that documentation somewhere that with a small amount of work, you can have a huge impact. And, and even if you make a mistake, let's say I, I wrote a paragraph, I make spelling mistake, nobody will die from there. I mean, that's so I think you can quickly merge that because most of the time this is just like a jump, you don't need to check it. \n",
      "\n",
      "34:58\n",
      "Host\n",
      "Right. I mean, you don't need to run it to run all the tests. You just see the and then, \n",
      "\n",
      "35:05\n",
      "Guest\n",
      "yeah, but this is so I just will pick it up those. But uh for sure is that we can be like reading and understanding what, what the impact. And uh of course, if this is a documentation that like work about mathematical aspects and stuff like that, it will be slower when it's straightforward changes. We don't need to go back and forth and having long discussion for speaking about nothing. So yeah, we try to be much more proactive on this. And for instance, uh we need to comment turners to review on very like aspects or meteorological aspects. Well, for documentation, we say we have a documentation team as well uh that uh are feeling like very like part of that process and then only a single maintenance can just merge. So for them like a part of a person from that team say, I mean, this is good enough for me. I see the value of that changes. So let's just go because uh that that will be already a net improvement. \n",
      "\n",
      "36:05\n",
      "Host\n",
      "Mhm And uh so speaking of contrib contributors, contributions. Um so I suspect you have a lot of people. Um so there are a lot of people who just contributed once and maybe twice. But uh and then they moved on with their lives. But you also have people who work full time on second learn, right? How many of those uh working full time on second learn are there? \n",
      "\n",
      "36:33\n",
      "Guest\n",
      "Um So, you know, now is one of the main uh like anti that pay for full time contributors. So uh in the open source team, we are now seven. So maybe five of the people will contribute to secular and then like, let's say in, in uh full time equivalent, because we spend our some time on our libraries uh to help out. Um And then there's Xenon people and then this is where this is a bit more easy to know if they are fully paid like full time. And uh usually there's one person at NVIDIA uh that uh that is team head, which is paying this way and otherwise it is more volunteer based things. So if people contribute to their time that are coded, but doesn't have like the full like day to work on that. So in total, I mean, like if you consider like the code, we might be top 15 people. Um And uh and this word, it is very important for us as well to be sure that for instance, the full time paid people, like what we have probable will not just take over like other people that those other people have the impression that they have no impact. Our goal is to be uh like the servant of, of the community. So we are here to uh address things that might be taking too much of your time if you are not, if you are a volunteer there. So we, we, we, we can do dirty work on C I, we can do like those tasks with which you might don't want to spend your time. I mean, if you don't like to, if you are really like to do that, I mean, no problems but we can freeze some time of, of other people. I mean, that's really how we see as well as some of our impacts that are being like a full time paid um contributor. \n",
      "\n",
      "38:29\n",
      "Host\n",
      "And when you mentioned that it, uh you need two commoners to look at a uh to approve a request. Uh I assume that the Coor are not just uh in real or probable in employees, they're also like these volunteers that uh can work somewhere outside. \n",
      "\n",
      "38:48\n",
      "Guest\n",
      "Yeah. So if, if two of these 15 people uh which are from the core, the core maintenance teams. Ok. So that's uh when, when it comes to like the maintenance of the program, the code is this core, core maintenance team. \n",
      "\n",
      "39:02\n",
      "Host\n",
      "OK. And uh like how does it actually look like uh for you? Because like if you go to github and open issues, there are 2000 issues. Like how do you and like how I cannot imagine how many pull requests are there every day that you need to review, like how do you manage all that? You probably have some sort of process for, for managing all these things. \n",
      "\n",
      "39:31\n",
      "Guest\n",
      "Of course, it's a chaotic process of open source. No. But um so it, it's after a joke because during a long time, we didn't have any process, we just like whoever want to look at an issue, just look at an issue, whoever want to review the, the pr uh review the pr because we cannot for and say to people that volunteers, please look at that thing and, and solve it because that's, that's, I mean, so at least in that now we have uh probable or for instance, the foundation Latin before we try to have a bit more of a process where we would have a, I mean, work on, on defining a committee road map. So help people that defend this committee road map. Then within the people that are paid, we can say we have our priority items on that road map and we will look at those items which uh and, and help on those basic things. And I assume that one thing that happens since pro bubbles uh is that nowadays, we went the step a bit further, which is we want to have project boards on, on github. And there is like project boards on priority that um the core team find that this is useful and like uh issue that are lingo is and, and to have an idea of like between milestone between really is what we should focus and what we try to achieve. I mean, this is like one way of organizing and having a bit more process, let's say. And the second one is that we create as well, this run Robin three issue pr So every week there is one contributor that say or understood \n",
      "\n",
      "41:14\n",
      "Host\n",
      "separately round dropping \n",
      "\n",
      "41:17\n",
      "Guest\n",
      "what, three years? Uh So three years uh task, let's say. So the idea is that you, it's, it's, it's not always the same person that we go there and it's like a weekly task where you go and you go on the issue tracker or you go on the pr uh and then you get feedback on things that have nobody received uh like any answer. And one of the issues if, if of not answering to people that they are leaving, but actually, these are part of our fault is that no, it is not only the fault of people just like living there that we don't have enough time like to, to retain them. So here we try to have a small process that we ensure that one person is affected to give back, like ask more details on the issues to, to know if we should close or not, if this is not particle if it's missing information. So this is like the easy part and the put a request is more on like giving a first feedback and, and to the person. So let's see if we can like uh quickly answer and that people doesn't get uh like I said, nobody cares about my, my, my contribution. I'm, I'm leaving. Mhm. And for the one, I think we are six or seven contributors doing this. So every six weeks this is coming back and, and uh well, before I think that it was much more like whoever want to answer or just answer And for instance, that was one of the thing. It's something that I really like, like going to, you should try answering to people and like interacting with them there, it's something that this is something that I like. But at the same time, you say, maybe I should as well spend time on some priority feature because I mean, this will have as well an impact. So I think here we on like the middle ground thing, we should have one person looking at it. But then like as well as a contributor, you should as well explain on community driven roadmap task your items. So that's um \n",
      "\n",
      "43:15\n",
      "Host\n",
      "I understand. OK. So you have to invest in community work as a, as a company, I guess as the core team that uh people don't feel like nobody cares about them. \n",
      "\n",
      "43:26\n",
      "Guest\n",
      "Yeah. So we uh we always give the example of uh for instance tens soft flow. And if you go on the pr we check if you go on the pr tracker of tens soft flow then you see bots committing because actually it's pr from the internal uh Google development that are just like back boarded on the external projects, but there is no community there. It, it is a open source software developed by your company uh which could be fine. I mean, that's answering the needs of, of uh of people is intents of role. But we have a very different uh target. We just want to be a community and, and grow around that. Uh And, and favor like people that want to actually be part of the process. I mean, is, is what we like is what we are from the beginning. Uh And it's not because probable is now a company that pays for the some developer that it should change. We are the service of the community, we are not driving the community uh by, by our choices. \n",
      "\n",
      "44:30\n",
      "Host\n",
      "And how often do you have to reject PRS? \n",
      "\n",
      "44:35\n",
      "Guest\n",
      "We, I mean, that's one of the reason of that we have 600 pr s is that we don't reject that many. Um uh let's say the, the, the most, the pr that we reject the most is people come in and say, oh look at my new fancy academic algorithms and I want to include it and then we say sorry, we cannot and but we came with credit to try to uh discuss basically the, the discussion which is we are very boring library. OK. So you will come, you can come back in three years and show us that people use your things. And this is actually have like a huge impacts on every data sentence like a two years. Yeah. So proof of time is what we need. So I assume this is most of the thing that we say we discarded because we create it some of uh two rules which is cations and, and let's say use like practical usefulness. They can show us that. Uh yeah, that's the way to go. Uh And you know, that things are slow in between. We can help at a for instance, making like imbalance never have been in section for those reasons. Um But this is a compatible package and we can tell you how to be compatible. We can give you the tools to put your C meter, be compatible and you maintain it until you show us that this is working. And for instance, in instance, H DB scan, which have been recently merging C is from uh Leland mcinnes uh like uh that wrote that inside the site contributions and that was compatible. And at the time that we saw that it has a clear uh like impact, it had been merged in psychic learn uh in this, in this way. \n",
      "\n",
      "46:23\n",
      "Host\n",
      "Mhm People usually talk to you in some way not to you specifically but to the core team before they start working on P RSI. Guess this is a good way. Of uh making sure that what you do before you create APR is actually not in vain that when you open APR, there is high chance of something happening, right? Uh So is there a way to somehow communicate with you to say, hey, like I have this idea. Do you think I should work on that? \n",
      "\n",
      "46:52\n",
      "Guest\n",
      "Yeah, that's the issue tracker. So the issue tracker is not only for issues. Yeah, like I want to propose this feature and we really like that. People start by that before, to me to, to send us APR usually what happened is that if you send APR and this is borderline, we didn't, we don't know exactly if we should go forward. We always as a person. So thank you. You will not receive any feedback here. Please open an issue so that we can have further discussions and then we'll come back there. If this is valuable, we don't close it necessarily because we don't want to be abrupt because we, we have no idea. I mean, if it's a clear thing that we see that here that's going to work, we are probably going to close, but we are very, we want to have the discussions and the issue tracker is as well the place for that. \n",
      "\n",
      "47:39\n",
      "Host\n",
      "Mhm Because uh one of the mistakes I personally did, I remember I tried to contribute to ex boost ex boost for Java specifically. And then I did that without talking first to the maintainers, I spent like maybe a week, not like full week, but like every now and then during this week, I spent some time and then I was so proud of opening in the pr and it was just rejected. And then, like, I was so disappointed, like I was uh like, I put so much effort there and I thought this is cool, this is improving the library. I'm doing something good and then they just, they had their own opinion on how things should be done, which is of course, this is who are going to to maintain it afterwards, right? So they it's ok for them to have this opinion. But for me as somebody who tried to, to make this contribution and my pr is closed, I feel kind of rejected, right? So now since then, I think like, OK, I first should talk to the maintainers to make sure that this aligns with what they want to see and if it does, then I move forward. \n",
      "\n",
      "48:49\n",
      "Guest\n",
      "Yeah. Yeah. I this is really like uh we, it's what we are writing inside our contribution, contribution guideline saying come to speak to us first. Uh we will not bite uh and even on, on the pr themselves, we are not going to bite at the first like, oh you open it like I just let we just close it without telling you why. I mean, we it's, it's not welcoming, we don't want people that feel rejected because, uh, we are just like, uh, boring and annoying people. Uh, we want to make people understand like, what are the reason? And, uh, and the process is long and it is a pity because the process could have saved you times must break if we add it up up front. I mean, that's, uh, but if you missed it, at least we want that. You understand what, what's the reason behind that is nothing personal is nothing that we don't give. Uh we don't care about it or I mean, it is just like, OK, let's, let's uh that's the same rule for everyone. We need to go through that, that, that discussion process. \n",
      "\n",
      "49:50\n",
      "Host\n",
      "So if I want to contribute to pr to second learn, what are the steps I should make to make sure that it actually happens or I don't spend my time, I don't waste my time implementing something that will not be accepted. \n",
      "\n",
      "50:05\n",
      "Guest\n",
      "So I think there's two things to do first is like read the contribution uh guideline, which is on the page. So it bring technical things but it brings as well like the aspect of uh what do we expect with the interaction, where, where to go and, and how to interact with the, with the community and then from there, yeah, it's going back to opening an issues and discussing and uh and then the way what we, we will let you know basically, but it's there is more details. We have always like a, a huge and too big page with information there but it's, it's where like it's um the gold mine to basically do that things. Mhm Yeah, I think nowadays we just like as well help a bit in the tracker. Like if you try to open an issue, then we tell you, do you want a bug report? Do you want an announcement feature? Do you want this and this? And then we tell you, oh, it fit this case, look at these things because if it's a bug, then we need a minimal reproducer and we show you how to do one. So we try to engage you to looking at the right part of the documentations. If you didn't come to the issue tracker, then yes, you didn't see that. Uh And then yeah, you might go like in the different path. Uh And that's completely fine as well is that we will bring you back to the to the path that we are used to. \n",
      "\n",
      "51:26\n",
      "Host\n",
      "Mhm So you also use some automation to make sure you can still manage all these uh issues and all this process \n",
      "\n",
      "51:36\n",
      "Guest\n",
      "kind of templating, let's say uh but we don't have any boats. Uh So for instance, project for instance, do like boat thing in activity during two months, three months, we just close that things and uh I'm personally really like against that. And uh uh and the thing is that if it's a bug. Uh, and then it's still a bug. I mean, it's not because you have an activity that you, to close it because, uh, either maybe you're not able to reproduce, but at least it's there. If somehow somebody else come with a reproducer later on, it's a win. If it's something close you have much is much more like, uh, difficult to interact because, like, you lost the sight of, of that, of that issue. So you can lose sight as well when you have 2000 issues. But I mean, that's uh uh but I mean, you have this, this so it, it's a kind of a trade off and the worst would be like if you log the conversations and saying now you cannot speak anymore, you have to open a new one, you need to make it easy so that people can come and just like discuss to you. And even if there is like noise but not related to what is a subject, I mean, this is there. That's, that's OK. I mean, \n",
      "\n",
      "52:50\n",
      "Host\n",
      "and if I just defended and if I just defended my phd with and developed like a novel method for something, I should probably wait 5 to 10 years before I make APR to second long. \n",
      "\n",
      "53:01\n",
      "Guest\n",
      "No, not 10 years and not five years. Uh And I would say it depends if your phd was to make a new server for making a linear model like uh optimization much faster, then it just come and tell us about it because that's an inner thing. It's not adding a new model, it's adding like a very internal things and maybe we will see and we have to discuss. So we are a bit more lenient on those like low level things that doesn't have like a user impact. It doesn't need to be really, I mean, first like the PR will be held for you because it will be apr to implement it, but it will be a lot of benchmark to be sure that every corner case that we are used to are covered and, and, and it will be a process that will take you a long time. I mean, it's not like uh I mean, you should not expect a one week pr because the process will, will take time. If this is like a full new algorithm, then yes, then like this is a much more complicated thing. \n",
      "\n",
      "54:09\n",
      "Host\n",
      "OK. Well, I have a question from Adonis. So Adonis is asking how do you decide on what to improve next in psychic learn? Is it public feedback trends? Something else? \n",
      "\n",
      "54:23\n",
      "Guest\n",
      "Yeah. So um a bit of, of everything. Um So first is we are slow to move inside land somewhere. Um So some of the priority that we have is priority that we got uh during like years and we just like improve over time. But then there's like, yeah new updates of what is important in 2024. Uh And uh and we have two ways of doing this. So first, like we're running this foundation where we had money from, sponsor company from outside that were our users as well. So we are asking them, how do you use Clan? And from there, we could have an idea of what's important for them and helping us at maybe updating and prioritizing item on, on, on the road map. So, going back to the committee with that feedback from our close users. Did you heard anything with like, is it, does it look like? OK, for you? Like this, is this, is it making sense or is it like some are really like only for that industry? And then we don't care. Uh So this this first things and now we are preparing with the community itself a much more public survey where we want to ask similar questions. So it takes much more, much more time because you need to prepare a survey which is short, but bring you the right information and this is like a, a job that you need to know how to do. So we are giving people like that really take care about like developing that survey. And I think it would be launched in a couple of weeks or upcoming months at most where we will ask like the broad community what are using second and for, and one of the goal of this service is for us to understand updating priority, another one as well. Like if the goals that we like the define since here are aligned with those one. So if you are a second user, you can expect at some point in the upcoming weeks you you will receive or you will see Delva like tweeting or the secular tweeting about this specific uh uh survey on the website, we will like maybe a top BN thing like click here if you want to help us to know and to shape the future of secular. \n",
      "\n",
      "56:41\n",
      "Host\n",
      "Hm So I was just going to ask you how we can stay notified and you mentioned that we can subscribe to uh a couple of Twitter accounts, right? \n",
      "\n",
      "56:53\n",
      "Guest\n",
      "Yeah. The second Twitter account is it will be like the main communication channels. Uh And then on the road map, it said the road map will be published on the on the website. So there's already one, if you go in the about section, then you would have something road map if I remember. And I always have this. I mean, it's not like static, it's moving over time. So here is the idea is to be able just to update it, but we need first to make the survey and, and consolidate it into actual things uh that we are working on. Uh Another source is, for instance, on the website, we are uh publishing um a road map of the priorities basically of the people that uh work on on uh on second to make it like obvious and transparent, what we're working on, what is our, our agenda. And actually on that broad post, there is not only ours there as well, like uh the agenda of uh other core contributors that decided to say, oh, I want as well to put my name on the list of tests because I will review all stuff like that. We, we want to make as well a communication on po birds sites to really show to people what we do. \n",
      "\n",
      "58:12\n",
      "Host\n",
      "Do you have uh something like a mailing list? Because Twitter is uh Twitter has an algorithm in order to decide to show you something or not. And even if I'm subscribed in, if I'm, if even if I'm following Cle the Cylon account, doesn't mean I will actually see it. \n",
      "\n",
      "58:29\n",
      "Guest\n",
      "So we have uh several things that's on social media. So we have um Twitter. Uh We have a huge pool on linkedin actually. So we will publish the same information on linkedin. And then we have this many in this which is a public one. like putting many that anybody can, can go there. We have low traffic and for instance, the roadmap will be surely something that we will communicate either to participate or either we publish a new road map, we will redirect, send an email saying like, oh go and see what's happening on the website because the items are there and we already for instance, do that for every releases saying, oh, we make a release, we use that uh C and mailing list and this public everybody can just like uh shift in. And um so that's like, like I would say that the free source of information and otherwise if you want to be close to the maintenance github is the best place for dis discussions maybe and, and uh issues. I mean, everything is socialize there uh in terms of like uh development \n",
      "\n",
      "59:38\n",
      "Host\n",
      "and I'm trying to find the mailing list now on circular website. So I see that there is a, a roadmap. Uh I don't see the mailing list uh or should I use maybe search on Clan? \n",
      "\n",
      "59:58\n",
      "Guest\n",
      "No, should I use? You'll be fine. Uh Because I think that we have a support. Yeah. So there is um there's a link on inside the support page. Uh \n",
      "\n",
      "60:12\n",
      "Host\n",
      "Second learn mailing list. I can see it \n",
      "\n",
      "60:16\n",
      "Guest\n",
      "and this is on the Python uh mailman basically. Uh So this is uh this looks \n",
      "\n",
      "60:22\n",
      "Host\n",
      "very old school, \n",
      "\n",
      "60:24\n",
      "Guest\n",
      "it's very old school. It is a job. \n",
      "\n",
      "60:27\n",
      "Host\n",
      "Yeah. Right. Ok. I think that's all we have time for today. So I really enjoyed this conversation. Thanks a lot for joining us today for sharing your experience, expertise, for talking about the internal uh of, of like behind the scenes uh of psych learn development. That was pretty interesting. So thanks for your, for your chat. \n",
      "\n",
      "60:50\n",
      "Guest\n",
      "Uh Happy to have been \n",
      "\n",
      "60:51\n",
      "Host\n",
      "here and Uh Thanks everyone for tuning in. So have a great week, everyone. Goodbye. Thanks a lot. Bye. \n"
     ]
    }
   ],
   "source": [
    "print(transcript_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aa18c8-f285-4be5-96d3-327fd304212e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
